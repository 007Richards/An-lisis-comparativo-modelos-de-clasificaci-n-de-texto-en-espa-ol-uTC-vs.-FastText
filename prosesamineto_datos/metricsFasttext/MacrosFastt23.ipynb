{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e3597f3b-5fcc-48c5-b42f-1b64a6444d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Líneas en test.txt: 65571\n"
     ]
    }
   ],
   "source": [
    "with open(\"predTyp.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "print(\"Líneas en test.txt:\", len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "712bc0d5-af5b-4d22-8603-ca5ac638e814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Líneas en test.txt: 65571\n"
     ]
    }
   ],
   "source": [
    "with open(\"Rest_Mex_Typ_test.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "print(\"Líneas en test.txt:\", len(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b5bb2236-57e2-4b16-9214-832e92e8a772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len y_true: 65571, Len y_pred: 65571\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "with open('predTyp.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        y_pred.append(line.strip().replace('__label__', ''))  # quitamos \"__label__\"\n",
    "\n",
    "# Tus verdaderos (etiquetas del test)\n",
    "y_true = []\n",
    "with open('Rest_Mex_Typ_test.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        etiqueta = line.split(' ')[0].replace('__label__', '')  # solo nos quedamos con la etiqueta\n",
    "        y_true.append(etiqueta)\n",
    "\n",
    "print(f\"Len y_true: {len(y_true)}, Len y_pred: {len(y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4565bbc8-ac98-4245-ac82-bbbc9f4910a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Accuracy: 0.7382\n",
      "\n",
      "F1-Score por clase:\n",
      "Clase Attractive: F1 = 0.9477\n",
      "Clase Hotel: F1 = 0.8692\n",
      "Clase Restaurant: F1 = 0.6669\n",
      "\n",
      "Macro F1-Score: 0.8279\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Macro-Accuracy\n",
    "macro_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Macro Accuracy: {macro_accuracy:.4f}\")\n",
    "\n",
    "# F1 por clase\n",
    "f1_por_clase = f1_score(y_true, y_pred, average=None, labels=['Attractive','Hotel','Restaurant'])\n",
    "\n",
    "print(\"\\nF1-Score por clase:\")\n",
    "for clase, f1 in zip(['Attractive','Hotel','Restaurant'], f1_por_clase):\n",
    "    print(f\"Clase {clase}: F1 = {f1:.4f}\")\n",
    "\n",
    "# Macro F1\n",
    "macro_f1 = f1_score(y_true, y_pred, average='macro', labels=['Attractive','Hotel','Restaurant'])\n",
    "print(f\"\\nMacro F1-Score: {macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc4d6e0-0867-4bcc-8866-86fcaf1b28a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
